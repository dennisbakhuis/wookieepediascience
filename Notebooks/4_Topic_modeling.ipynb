{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3212639-5b12-4a00-85d8-b95fc580cb9f",
   "metadata": {},
   "source": [
    "# Star Wars Data Science\n",
    "## Network Analysis, Topic Modeling, and a Wordcloud!\n",
    "https://linkedin.com/in/dennisbakhuis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c4051a-760b-4383-9474-0016d8e0bc17",
   "metadata": {},
   "source": [
    "## 4. Star Wars Topic Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c705bd-e371-4acc-a499-6bf39de30990",
   "metadata": {},
   "source": [
    "Until now we have covered the basics with some data exploration and we have created the mandatory wordcloud. Therefore, we are now ready to go into the more advanced analysis and the first I want to give a try in topic modeling. The method described here is based on the work by Maarten Grootendorst, the creator of BerTopic.\n",
    "\n",
    "Topic modeling is an unsupervised learning technique that can answer the following question: I have this bunch texts, what are the most common topics these text talk about. \n",
    "\n",
    "For topic modeling we will make use of a Python package called Sentence-Transformers which is as the name suggests based on the transformers architecture and is able to convert a full sentence into a vector. To find similar topics, we need to find vectors that are grouped together. Lets first create the sentence embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf002c5-47b1-4ceb-b6b5-4b3d07d239dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import umap\n",
    "import hdbscan\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1931838-24e2-4c01-bda3-2c726116ab72",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = pd.read_parquet('../Dataset/StarWars_Raw_Sentences.parquet')\n",
    "model = SentenceTransformer('distilbert-base-nli-mean-tokens')\n",
    "embeddings = model.encode(sent.sentence, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2538ea4e-4af9-4152-8aaf-23f0daff17fd",
   "metadata": {},
   "source": [
    "The sentence vectors have a length of 768 which is very large to to the cluster analysis, therefore, we will apply a dimension reduction. There are many choices, such as LDA or NMF, but here we will use a method called Umap which has the benefit of keeping the local structure in tact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a874a8c8-72e9-4cb8-8c92-2c127f86a44d",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components = 6\n",
    "n_neighbors = 24\n",
    "\n",
    "umap_embeddings = umap.UMAP(\n",
    "    n_neighbors=n_neighbors, \n",
    "    n_components=n_components, \n",
    "    metric='cosine',\n",
    ").fit_transform(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8debb2-4368-4e0f-b5e6-67a7f7bcee1b",
   "metadata": {},
   "source": [
    "After the dimension reduction we can try to identify the clusters using Hdbscan. This will find the clusters that are grouped together in the reduced parameter space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c92f63f-2140-4b15-8d2d-1f2ab15d3cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = hdbscan.HDBSCAN(\n",
    "    min_cluster_size=n_neighbors,\n",
    "    metric='euclidean',                      \n",
    "    cluster_selection_method='eom',\n",
    ").fit(umap_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a5864d-a88d-4148-a7b8-d5924c03cb2f",
   "metadata": {},
   "source": [
    "We have reduced the 768 features to only 6 features, however this reduction is not enough if we want to plot this to the screen. For this we need to reduce it at least to 3 dimensions and we will go even down to 2 for better visibility. For this we are again using Umap and reduce the original dataset from 768 all the way to 2 and combine the earlier defined label with the coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69be83c1-9205-4010-b706-7555510d4b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame\n",
    "plot_df = umap.UMAP(\n",
    "    n_neighbors=n_neighbors,\n",
    "    n_components=2,\n",
    "    min_dist=0.0,\n",
    "    metric='cosine'\n",
    ").fit_transform(embeddings)\n",
    "\n",
    "result = pd.DataFrame(plot_df, columns=['x', 'y'])\n",
    "result['label'] = cluster.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c761ea-954a-4eda-a18f-5a3ac7878e15",
   "metadata": {},
   "source": [
    "And now we can plot the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec6126e-86cf-4d29-a4ba-a1dc24b33f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "outliers = result.loc[result.label == -1, :]  # cluster -1 are \"outliers\"\n",
    "clustered = result.loc[result.label != -1, :]\n",
    "\n",
    "# plot outliers is gray\n",
    "plt.scatter(\n",
    "    outliers.x,\n",
    "    outliers.y,\n",
    "    color='#BDBDBD',\n",
    "    s=0.05,\n",
    ")\n",
    "\n",
    "# plot clusters in color\n",
    "plt.scatter(\n",
    "    clustered.x,\n",
    "    clustered.y,\n",
    "    c=clustered.label,\n",
    "    s=0.05, \n",
    "    cmap='hsv_r',\n",
    ")\n",
    "_, _ = ax.set_xlim([3, 15]), ax.set_ylim([4, 15])\n",
    "_, _ = ax.set_xlabel('umap_1'), ax.set_ylabel('umap_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4466a5-014f-463b-b45b-f702d3ff55fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.savefig('../Assets/sw_topic1.png', bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9231d63d-aa2a-40e6-ba79-59a24462811c",
   "metadata": {},
   "source": [
    "The result is very pretty but to get some idea if these clusters make any sense we need to combine the clusters with their original sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070146f6-732b-4bcf-9754-0f7e3c71f086",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_df = pd.DataFrame({\n",
    "    'Doc': sent.sentence.tolist(),\n",
    "    'Topic': cluster.labels_,\n",
    "    'Doc_ID': sent.index.tolist(),\n",
    "})\n",
    "\n",
    "# Combine all documents for each topic\n",
    "docs_per_topic = (docs_df\n",
    "    .groupby(['Topic'], as_index = False)\n",
    "    .agg({'Doc': ' '.join})\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf97f18-ddec-4e0f-b974-0d1a578c8254",
   "metadata": {},
   "source": [
    "The result is very pretty but to get some idea if these clusters make any sense we need to combine the clusters with their original sentences. Also, we want to find the most common keywords for each cluster and therefore, we need to analyze each cluster separately. To solve this, Maarten Grootendorst came up with a class based TF_IDF which works pretty neat!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e21b8644-65ec-4d80-b7af-62f0039dfb27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def c_tf_idf(documents, m, ngram_range=(1, 1)):\n",
    "    count = CountVectorizer(\n",
    "        ngram_range=ngram_range,\n",
    "        stop_words=\"english\",\n",
    "    ).fit(documents)\n",
    "    \n",
    "    t = count.transform(documents).toarray()\n",
    "    w = t.sum(axis=1)\n",
    "    tf = np.divide(t.T, w)\n",
    "    sum_t = t.sum(axis=0)\n",
    "    idf = np.log(np.divide(m, sum_t)).reshape(-1, 1)\n",
    "    tf_idf = np.multiply(tf, idf)\n",
    "\n",
    "    return tf_idf, count\n",
    "  \n",
    "tf_idf, count = c_tf_idf(docs_per_topic.Doc.values, m=len(sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4835fdc0-96b7-48e9-90a2-96a2607ac93e",
   "metadata": {},
   "source": [
    "We need to apply c_tf_idf for each cluster and treat the collection of documents as a single document. The result is a list of most frequent terms and hopefully they make some sense on our Wookieepedia dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd00538-ed1e-498b-b89a-3d5c16f23063",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_top_n_words_per_topic(\n",
    "    tf_idf, \n",
    "    count, \n",
    "    docs_per_topic, \n",
    "    n=20\n",
    "):\n",
    "    words = count.get_feature_names()\n",
    "    labels = list(docs_per_topic.Topic)\n",
    "    tf_idf_transposed = tf_idf.T\n",
    "    indices = tf_idf_transposed.argsort()[:, -n:]\n",
    "    top_n_words = {\n",
    "        label: [\n",
    "            (words[j], tf_idf_transposed[i][j]) \n",
    "            for j in indices[i]\n",
    "        ][::-1] \n",
    "        for i, label in enumerate(labels)\n",
    "    }\n",
    "    return top_n_words\n",
    "\n",
    "def extract_topic_sizes(df):\n",
    "    return (df\n",
    "        .groupby(['Topic'])\n",
    "        .Doc\n",
    "        .count()\n",
    "        .reset_index()\n",
    "        .rename({\"Topic\": \"Topic\", \"Doc\": \"Size\"}, axis='columns')\n",
    "        .sort_values(\"Size\", ascending=False)\n",
    "    )\n",
    "\n",
    "top_n_words = extract_top_n_words_per_topic(\n",
    "    tf_idf,\n",
    "    count,\n",
    "    docs_per_topic,\n",
    "    n=20,\n",
    ")\n",
    "topic_sizes = extract_topic_sizes(docs_df)\n",
    "topic_sizes.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ceb9cc-0c79-41f4-b5ce-bd944f538248",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_n_words[93]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc9f662c-468c-4fe0-9507-b927cb33aa47",
   "metadata": {},
   "source": [
    "When looking at the results there are actually some that make sense but are not that cool. There is a full cluster that has the many colors, which is still pretty cool, knowing that it got it from an unsupervised way. There are however a couple of clusters that are indeed have story-based topics. For example, a cluster containing Emperor Palpatine contained lots of political terms such as senate, supreme chancellor, and constitution. Palatine also had links to clones which is the master plan he created to destroy the Jedi."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c77f7e63-97cd-40d7-91e4-7b3adb8f966f",
   "metadata": {},
   "source": [
    "However my most favorite cluster combines almost all star characters together with the terms missions and battle of Yavin. Of course, the battle of Yavin is one of the most important events in Star Wars and it makes sense that it is often referenced. Still, finding this using topic modeling is pretty awesome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c15f370d-2425-4a03-85ac-af72af0cf58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ix, group in top_n_words.items():\n",
    "    if any([True if x[0]=='skywalker' else False for x in group]):\n",
    "        print(ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c16dceb-d99c-4b72-b4ce-4c0393d5c087",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_n_words[162]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5fc0044-c060-4413-b32d-449943dc415d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ix, group in top_n_words.items():\n",
    "    if any([True if x[0]=='palpatine' else False for x in group]):\n",
    "        print(ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353bd75d-af80-4459-b243-518d121bc3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_n_words[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8253c1-1a36-49dd-a9c9-6390d25fe577",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8edfde6b-2c2e-4a2f-8885-be5a463e44b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6431035d-a959-4f66-9602-9a8b8c1b3cc5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
