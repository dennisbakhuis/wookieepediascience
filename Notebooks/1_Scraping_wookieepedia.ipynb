{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a47e529c-d1d6-4b3d-9206-4573bd1968b6",
   "metadata": {},
   "source": [
    "# Star Wars Data Science\n",
    "## Network Analysis, Topic Modeling, and a Wordcloud!\n",
    "https://linkedin.com/in/dennisbakhuis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "charitable-guitar",
   "metadata": {},
   "source": [
    "## Scraping and building the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "silver-comedy",
   "metadata": {},
   "source": [
    "There is an incredible amount of information on Star Wars online. One of my favorite sources is the so called Wookieepedia, a wiki with a crazy amount of Star Wars knowledge. \n",
    "\n",
    "`All data is available in the Github repository and Kaggle so there is no need to scrape it yourself and generate high amount of traffic for Wookieepedia.`\n",
    "\n",
    "In this section describes the process to scrape this information. A wiki is a collection of pages and each topic has its own page. To scrape this information, we need to visit each page. There is a clever way to scrape such websites and that is by using Sitemaps. It is a special file that webmasters can provide that will help web crawlers with indexing the website. We can make use of the sitemap to get a list of all the pages that are available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "instructional-momentum",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "url = 'https://starwars.fandom.com'\n",
    "\n",
    "def get_elements(url : str) -> dict:\n",
    "    site_map_str = \"/sitemap-newsitemapxml-index.xml\"\n",
    "    result = requests.get(url + site_map_str)\n",
    "    content = result.content\n",
    "    \n",
    "    root = ET.fromstring(content)\n",
    "    elements = {}\n",
    "    for page in root.iter('{http://www.sitemaps.org/schemas/sitemap/0.9}loc'):\n",
    "        result = requests.get(page.text)\n",
    "        c = result.content\n",
    "        new_root = ET.fromstring(c)\n",
    "        for element in new_root.iter('{http://www.sitemaps.org/schemas/sitemap/0.9}loc'):\n",
    "            elements[element.text.split('/')[-1]] = element.text\n",
    "    print('Found {} elements'.format(len(elements)))\n",
    "    return elements\n",
    "\n",
    "elements = get_elements(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f5d597-17c7-423b-8e99-48364d4db0de",
   "metadata": {},
   "source": [
    "Currently (April 2021) there are 219.900 pages that could be scraped. This is however a bit too much. Therefore, I decided to only scrape the pages that are considered canon. Luckily, Wookieepedia gave canon articles their own category. When we click on the category, we get a paginated index of all pages that are considered canon. This needs some additional work so that we are able to scrape all the topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "structured-landing",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup \n",
    "import pickle\n",
    "\n",
    "page_url = 'https://starwars.fandom.com/wiki/Category:Canon_articles'  # all canon articles\n",
    "base_url = 'https://starwars.fandom.com'\n",
    "\n",
    "pages = {}\n",
    "page_num = 1\n",
    "while page_url is not None:\n",
    "    result = requests.get(page_url)\n",
    "    content = result.content\n",
    "    soup = BeautifulSoup(content, \"html.parser\")\n",
    "    \n",
    "    # extract urls\n",
    "    links = soup.find_all('a', class_='category-page__member-link')\n",
    "    links_before = len(pages)\n",
    "    if links:\n",
    "        for link in links:\n",
    "            url = base_url + link.get('href')\n",
    "            key = link.get('href').split('/')[-1]\n",
    "            if 'Category:' not in key:\n",
    "                pages[key] = url\n",
    "    print(f'Page {page_num} - {len(pages) - links_before} new links ({page_url})')\n",
    "    page_num += 1\n",
    "                \n",
    "    # get next page button\n",
    "    next_urls = soup.find_all(\"a\", class_='category-page__pagination-next')\n",
    "    if next_urls:\n",
    "        new_url = next_urls[0].get('href')\n",
    "        if new_url == page_url:\n",
    "            break\n",
    "        else:\n",
    "            page_url = new_url\n",
    "    else:\n",
    "        puge_url = None\n",
    "\n",
    "\n",
    "print(f'Number of pages: {len(pages)}')\n",
    "\n",
    "# Save to disk\n",
    "with open('../Dataset/starwars_all_canon_dict.pickle', 'wb') as f:\n",
    "    pickle.dump(pages, f, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2927f3ff-b1e2-4146-94a3-e5483c4d2ec2",
   "metadata": {},
   "source": [
    "Alright, we now have a list of 29k pages that are considered canon which need to be scraped. These pages have a typical format consisting of a title, a description often with subsections, and a sidebar with properties. To reduce the amount of information, I will only scrape the the first paragraph, the full sidebar, and all links that point towards other canon pages. A typical Wookieepedia page is shown in figure x:\n",
    "\n",
    "<img src=\"../Assets/sw_scrape1.png\" alt=\"Artificial Neural Network example\" width=\"500\" style=\"display: block; margin: 0 auto\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e372fff3-a842-424a-a853-b44bb4a414bd",
   "metadata": {},
   "source": [
    "Next, we scrape each page and save the partitioned to disk:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "promising-ordinary",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "scraped = {}\n",
    "failed = {}\n",
    "partition_size = 5000\n",
    "folder = '../Dataset/'\n",
    "!rm -rf ./data\n",
    "!mkdir -p ./data\n",
    "\n",
    "for ix, (key, page_url) in tqdm(enumerate(pages.items()), total=(len(pages))):\n",
    "    try:\n",
    "        # Get page\n",
    "        result = requests.get(page_url)\n",
    "        content = result.content\n",
    "        soup = BeautifulSoup(content, \"html.parser\")\n",
    "\n",
    "        # Get title\n",
    "        heading = soup.find('h1', id='firstHeading')\n",
    "        if heading is None: continue\n",
    "        heading = heading.text\n",
    "\n",
    "        # Extract Sidebar\n",
    "        is_character = False\n",
    "        side_bar = {}\n",
    "        sec = soup.find_all('section', class_='pi-item')\n",
    "        for s in sec:\n",
    "            title = s.find('h2')\n",
    "            if title is None:\n",
    "                title = '<no category>'\n",
    "            else:\n",
    "                title = title.text\n",
    "            side_bar[title] = {}\n",
    "            items = s.find_all('div', class_='pi-item')\n",
    "            for item in items:\n",
    "                attr = item.find('h3', class_='pi-data-label')\n",
    "                if attr is None:\n",
    "                    attr = '<no attribute>'\n",
    "                else:\n",
    "                    attr = attr.text\n",
    "                if attr == 'Species': is_character = True\n",
    "                value = re.sub(\"[\\(\\[].*?[\\)\\]]\" ,'', '], '.join(item.find('div', class_='pi-data-value').text.split(']')))\n",
    "                value = value.strip()[:-1].replace(',,', ',')\n",
    "                if ',' in value:\n",
    "                    value = [i.strip() for i in value.split(',') if i.strip() != '']\n",
    "                side_bar[title][attr] = value\n",
    "\n",
    "        # Raw page content\n",
    "        raw_content = soup.find('div', class_='mw-parser-output')\n",
    "        if raw_content is not None:\n",
    "            for raw_paragraph in raw_content.find_all('p', recursive=False):\n",
    "                if 'aside' in str(raw_paragraph): continue\n",
    "                break\n",
    "            paragraph = value = re.sub(\"[\\(\\[].*?[\\)\\]]\" ,'', raw_paragraph.text)\n",
    "\n",
    "            # cross-links\n",
    "            keywords = []\n",
    "            for link in raw_content.find_all('a'):\n",
    "                part = link.get('href')\n",
    "                if part is not None:\n",
    "                    part = part.split('/')[-1] \n",
    "                    if part in pages.keys() and part != key:\n",
    "                        keywords.append(part)\n",
    "            keywords = list(set(keywords))\n",
    "        else:\n",
    "            # Empty page\n",
    "            keywords = []\n",
    "            paragraph = ''\n",
    "\n",
    "        # Data object\n",
    "        scraped[key] = {\n",
    "            'url': page_url,\n",
    "            'title': heading,\n",
    "            'is_character': is_character,\n",
    "            'side_bar': side_bar,\n",
    "            'paragraph': paragraph,\n",
    "            'crosslinks': keywords,\n",
    "        }\n",
    "\n",
    "        # save partition\n",
    "        if (ix + 1) % partition_size == 0:\n",
    "            last_number = (ix+1) // partition_size\n",
    "            fn = folder + f'starwars_all_canon_data_{last_number}.pickle'\n",
    "            with open(fn, 'wb') as f:\n",
    "                pickle.dump(scraped, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "            scraped = {}\n",
    "    except:\n",
    "        print('Failed!')\n",
    "        failed[key] = page_url\n",
    "    \n",
    "# Save final part to disk\n",
    "fn = folder + f'starwars_all_canon_data_{last_number + 1}.pickle'\n",
    "with open(fn, 'wb') as f:\n",
    "    pickle.dump(scraped, f, protocol=pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e05f9b39-9396-4f1f-83e5-7957c6358763",
   "metadata": {},
   "source": [
    "It took a little bit more than an hour to scrape all information and all is stored in pickle files that are split in sections of max 5000 pages. This is the raw dataset and we can always fall back to this.\n",
    "\n",
    "Next we will split the raw data in two sections: characters and raw text sentences. The character are identified by a property called 'species' that is available in the sidebar. We will collect all characters in a strongly structured Pandas DataFrame and this means that we need to select properties beforehand. We have a total of 5334 characters that are marked as canon. More details on the selected properties are in the scrape notebook that can be found in the Github repository.\n",
    "The raw text sentences are extracted from the descriptions. Each description is split into sentences and collected in a single list. Details of this split is can also be found in scrape notebook on Github.It took a little bit more than an hour to scrape all information and all is stored in pickle files that are split in sections of max 5000 pages. This is the raw dataset and we can always fall back to this.\n",
    "\n",
    "Next we will split the raw data in two sections: characters and raw text sentences. The character are identified by a property called 'species' that is available in the sidebar. We will collect all characters in a strongly structured Pandas DataFrame and this means that we need to select properties beforehand. We have a total of 5334 characters that are marked as canon. More details on the selected properties are in the scrape notebook that can be found in the Github repository.\n",
    "The raw text sentences are extracted from the descriptions. Each description is split into sentences and collected in a single list. Details of this split is can also be found in scrape notebook on Github."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "settled-wellington",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import urllib\n",
    "\n",
    "\n",
    "files = sorted(Path('../Dataset').glob('*.pickle'))\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mature-reviewer",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {}\n",
    "for fn in files:\n",
    "    with open(fn, 'rb') as f:\n",
    "        part = pickle.load(f)\n",
    "    data.update(part)\n",
    "\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db07509-cdd2-43c2-ab88-667ccfba52af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_url_shizzle(text):\n",
    "    return urllib.parse.unquote(text).replace('\"', '').replace(\"'\", '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c951c692-a25b-47bb-965a-d6c1988adb6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned = {}\n",
    "for key, value in tqdm(data.items()):\n",
    "    new_key = remove_url_shizzle(key)\n",
    "    cleaned[new_key] = value\n",
    "    cleaned[new_key]['crosslinks'] = [remove_url_shizzle(crosslink) for crosslink in value['crosslinks']]\n",
    "data = cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb2a92dc-064d-4f43-bd96-7e128cef4c51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "239b76da-88b8-4d7a-aa81-71cbf2b69a67",
   "metadata": {},
   "source": [
    "### Star Wars character dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "atlantic-marketing",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_key(key_name, data):\n",
    "    for key, value in data.items():\n",
    "        if key_name == key:\n",
    "            return value\n",
    "        if isinstance(value, dict):\n",
    "            value = find_key(key_name, value)\n",
    "            if value is not None:\n",
    "                return value\n",
    "    return None\n",
    "\n",
    "def get_first(key_name, data):\n",
    "    result = find_key(key_name, data)\n",
    "    if isinstance(result, list):\n",
    "        result = result[0]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "asian-toner",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "result = []\n",
    "for key, part in data.items():\n",
    "    if not part['is_character']: continue\n",
    "    row = {\n",
    "        'name': part['title'],\n",
    "        'key': key,\n",
    "        'url': part['url'],\n",
    "        'description': part['paragraph']\n",
    "    }\n",
    "    \n",
    "    species  = find_key('Species', part['side_bar'])\n",
    "    row['species_2nd'] = None\n",
    "    row['species_3rd'] = None\n",
    "    if isinstance(species, list):\n",
    "        row['species'] = species[0]\n",
    "        if len(species) > 1:\n",
    "            row['species_2nd'] = species[1]\n",
    "        if len(species) > 2:\n",
    "            row['species_3rd'] = species[2]\n",
    "        if len(species) > 3:\n",
    "            print(species)\n",
    "    else:\n",
    "        row['species'] = species.strip()\n",
    "    row['home_world'] = get_first('Homeworld', part['side_bar'])\n",
    "    row['gender'] = get_first('Gender', part['side_bar'])\n",
    "\n",
    "    row['height'] = get_first('Height', part['side_bar'])\n",
    "    row['eye_color'] = get_first('Eye color', part['side_bar'])\n",
    "    row['skin_color'] = get_first('Skin color', part['side_bar'])\n",
    "    row['hair_color'] = get_first('Hair color', part['side_bar'])\n",
    "    row['weight'] = get_first('Mass', part['side_bar'])\n",
    "\n",
    "        \n",
    "    \n",
    "    result.append(row)\n",
    "df = pd.DataFrame(result)\n",
    "\n",
    "# fix gender some errors\n",
    "gender_map = {\n",
    "    'Male': 'Male',\n",
    "    'Female': 'Female',\n",
    "    'Mal': 'Male',\n",
    "    'Femal': 'Female',\n",
    "    'Non-binary': 'Non-binary',\n",
    "    'male': 'Male',\n",
    "    'Males': 'Male',\n",
    "    'female': 'Female',\n",
    "    'Femle': 'Female',\n",
    "}\n",
    "df.loc[:, 'gender'] = df.gender.map(gender_map)\n",
    "df['gender'] = df['gender'].fillna('None')\n",
    "\n",
    "# normalize height\n",
    "translate = {None: None}\n",
    "for m in df.height.unique().tolist()[1:]:\n",
    "    if 'meter' in m:\n",
    "        try:\n",
    "            split = m.split()\n",
    "            if len(split) == 2:\n",
    "                if '/' in split[0]:\n",
    "                    split[0] = split[0].split('/')[0]\n",
    "                translate[m] = float(split[0])\n",
    "            elif split[0] == 'Around' or split[0] == 'Over':\n",
    "                translate[m] = float(split[1])\n",
    "            elif split[0] == 'At':\n",
    "                translate[m] = float(split[2])\n",
    "            elif split[-1] == 'shoulder':\n",
    "                translate[m] = float(split[0])\n",
    "            elif split[-1] == 'meters':\n",
    "                translate[m] = float(split[-2])\n",
    "            elif split[1] == 'millimeters':\n",
    "                translate[m] = 1.7015\n",
    "            elif split[1] == 'meters':\n",
    "                translate[m] = float(split[0])\n",
    "            else:\n",
    "                print(split)\n",
    "                break\n",
    "        except:\n",
    "            print(m)\n",
    "            break\n",
    "    elif 'feet' in m or 'ft' in m:\n",
    "        try:\n",
    "            split = m.split()\n",
    "            if split[0] == 'Around' or split[0] == 'Almost':\n",
    "                translate[m] = 0.3 * int(split[1])\n",
    "            elif len(split) == 4:\n",
    "                translate[m] = 0.3 * int(split[0]) + 0.0254 * int(split[2])\n",
    "            elif len(split) == 2:\n",
    "                translate[m] = 0.3 * int(split[0])\n",
    "            else:\n",
    "                print(split)\n",
    "                break\n",
    "        except:\n",
    "            print(m)\n",
    "            break     \n",
    "    elif m[-1] == 'c':\n",
    "        translate[m] = float(m[:-1]) / 100\n",
    "    elif m == '5:1':\n",
    "        translate[m] = None\n",
    "    else:\n",
    "        try:\n",
    "            translate[m] = float(m)\n",
    "        except:\n",
    "            print(m)\n",
    "            break     \n",
    "df['height'] = df.height.map(translate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a8ee9fe-91ec-460a-8b65-47415b5a34af",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_parquet('../Dataset/StarWars_Characters.parquet', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c32f8a8-41b8-4346-acf0-2be7c7a5e071",
   "metadata": {},
   "source": [
    "### Raw sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f1125b-59c2-464c-92df-6768fe74d52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fd = pd.DataFrame([\n",
    "    {\n",
    "        'key': key,\n",
    "        'title': value['title'],\n",
    "        'is_character': value['is_character'],\n",
    "        'description': value['paragraph'],\n",
    "    } for key, value in data.items()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab8c0d7-7648-4a6c-8b92-4193240278df",
   "metadata": {},
   "outputs": [],
   "source": [
    "fd.to_parquet('../Dataset/StarWars_Descriptions.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e87ac24-b90e-4d8a-83cb-be5c093cba9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from tqdm import tqdm\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a6ed5d-7d03-49b5-b6c4-f479abb93c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_split(text):\n",
    "    doc = nlp(text)\n",
    "    return [sent.text for sent in doc.sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cdedd4b-38b8-40b7-8ea8-1b38a202ba0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "sentences = []\n",
    "for description in tqdm(fd.description.values):\n",
    "    sentences += sentence_split(description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8c2606-d322-4303-bad7-892a739f5e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = pd.DataFrame(sentences, columns=['sentence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ddc9d48-f257-4c39-8abe-98c617f5684e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent.to_parquet('../Dataset/StarWars_Raw_Sentences.parquet', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb419ff3-3510-4d2f-bc4b-184a4e437528",
   "metadata": {},
   "source": [
    "### Raw sentences for characters only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bfb3421-26ad-4563-bf54-22d3b14b3e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "sentences2 = []\n",
    "for description in tqdm(fd.loc[fd.is_character, 'description'].values):\n",
    "    sentences2 += sentence_split(description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "354f171d-040b-4f33-9d77-77685552fd14",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent2 = pd.DataFrame(sentences2, columns=['sentence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a218d9-6b49-4fbc-a7a0-abf1ac706347",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent2.to_parquet('../Dataset/StarWars_Raw_Sentences_characters.parquet', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8188675-1f97-4531-b30a-2cfdfe1f76f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a7278b4-a77d-4913-a1a1-ea26f5411452",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
